{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j30cGm1Cyh0s"
   },
   "source": [
    "**You may need to install [OpenCV](https://pypi.python.org/pypi/opencv-python) and [scikit-video](http://www.scikit-video.org/stable/).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9328,
     "status": "ok",
     "timestamp": 1579630411605,
     "user": {
      "displayName": "Léo Andéol",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCZ9lXKogam3N-Zp1ZulY_dPre1MZo_DYuijwWz=s64",
      "userId": "15347828844155644240"
     },
     "user_tz": -60
    },
    "id": "0DzX-9DcgpQ_",
    "outputId": "4e1ecdaa-9285-4872-868c-232d2c4bb107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-video in /home/leo/.local/lib/python3.7/site-packages (1.1.11)\n",
      "Requirement already satisfied: opencv-python in /home/leo/.local/lib/python3.7/site-packages (4.1.1.26)\n",
      "Requirement already satisfied: numpy in /home/leo/.local/lib/python3.7/site-packages (from scikit-video) (1.18.0)\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from scikit-video) (5.4.1)\n",
      "Requirement already satisfied: scipy in /home/leo/.local/lib/python3.7/site-packages (from scikit-video) (1.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-video opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_jsWDgVyh0z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "import skvideo.io\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "from keras.models import Sequential,model_from_json\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, AveragePooling2D,Reshape,BatchNormalization, Flatten\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# due to GPU issue on my computer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6HMdaHGyh1B"
   },
   "source": [
    "# MiniProject on Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7z4XxDgyh1D"
   },
   "source": [
    "__Notations__: $E_p$ is the expectation under probability $p$. Please justify each of your answer and widely comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wP3alIHvyh1F"
   },
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJL_oNevyh1J"
   },
   "source": [
    "In a reinforcement learning algorithm, we modelize each step $t$ as an action $a_t$ obtained from a state $s_t$, i.e. $\\{(a_{t},s_{t})_{t\\leq T}\\}$ having the Markov property. We consider a discount factor $\\gamma \\in [0,1]$ that ensures convergence. The goal is to find among all the policies $\\pi$, one that maximizes the expected reward:\n",
    "\n",
    "\\begin{equation*}\n",
    "R(\\pi)=\\sum_{t\\leq T}E_{p^{\\pi}}[\\gamma^t r(s_{t},a_{t})] \\> ,\n",
    "\\end{equation*}\n",
    "\n",
    "where: \n",
    "\\begin{equation*}p^{\\pi}(a_{0},a_{1},s_{1},...,a_{T},s_{T})=p(a_{0})\\prod_{t=1}^{T}\\pi(a_{t}|s_{t})p(s_{t+1}|s_{t},a_{t}) \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "We note the $Q$-function:\n",
    "\n",
    "\\begin{equation*}Q^\\pi(s,a)=E_{p^{\\pi}}[\\sum_{t\\leq T}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a] \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "Thus, the optimal Q function is:\n",
    "\\begin{equation*}\n",
    "Q^*(s,a)=\\max_{\\pi}Q^\\pi(s,a) \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "In this project, we will apply the deep reinforcement learning techniques to a simple game: an agent will have to learn from scratch a policy that will permit it maximizing a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_xRUV_dyh1L"
   },
   "source": [
    "## The environment, the agent and the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aW9S9QHPyh1N"
   },
   "source": [
    "### The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CN6Zc0O6yh1P"
   },
   "source": [
    "```Environment``` is an abstract class that represents the states, rewards, and actions to obtain the new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kft0EqhVyh1T"
   },
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, act):\n",
    "        \"\"\"\n",
    "        One can act on the environment and obtain its reaction:\n",
    "        - the new state\n",
    "        - the reward of the new state\n",
    "        - should we continue the game?\n",
    "\n",
    "        :return: state, reward, game_over\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reinitialize the environment to a random state and returns\n",
    "        the original state\n",
    "\n",
    "        :return: state\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        Visualize in the console or graphically the current state\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6xgUv45yh1f"
   },
   "source": [
    "The method ```act``` allows to act on the environment at a given state $s_t$ (stored internally), via action $a_t$. The method will return the new state $s_{t+1}$, the reward $r(s_{t},a_{t})$ and determines if $t\\leq T$ (*game_over*).\n",
    "\n",
    "The method ```reset``` simply reinitializes the environment to a random state $s_0$.\n",
    "\n",
    "The method ```draw``` displays the current state $s_t$ (this is useful to check the behavior of the Agent).\n",
    "\n",
    "We modelize $s_t$ as a tensor, while $a_t$ is an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5JyJnchyh1i"
   },
   "source": [
    "### The Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3zfSFexyh1k"
   },
   "source": [
    "The goal of the ```Agent``` is to interact with the ```Environment``` by proposing actions $a_t$ obtained from a given state $s_t$ to attempt to maximize its __reward__ $r(s_t,a_t)$. We propose the following abstract class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lc8I-5iOyh1m"
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, epsilon=0.1, n_action=4):\n",
    "        self.epsilon = epsilon\n",
    "        self.n_action = n_action\n",
    "    \n",
    "    def set_epsilon(self,e):\n",
    "        self.epsilon = e\n",
    "\n",
    "    def act(self,s,train=True):\n",
    "        \"\"\" This function should return the next action to do:\n",
    "        an integer between 0 and 4 (not included) with a random exploration of epsilon\"\"\"\n",
    "        if train:\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                a = np.random.randint(0, self.n_action, size=1)[0]\n",
    "            else:\n",
    "                a = self.learned_act(s)\n",
    "        else: # in some cases, this can improve the performance.. remove it if poor performances\n",
    "            a = self.learned_act(s)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def learned_act(self,s):\n",
    "        \"\"\" Act via the policy of the agent, from a given state s\n",
    "        it proposes an action a\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reinforce(self, s, n_s, a, r, game_over_):\n",
    "        \"\"\" This function is the core of the learning algorithm. \n",
    "        It takes as an input the current state s_, the next state n_s_\n",
    "        the action a_ used to move from s_ to n_s_ and the reward r_.\n",
    "        \n",
    "        Its goal is to learn a policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" This function returns basic stats if applicable: the\n",
    "        loss and/or the model\"\"\"\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\" This function allows to restore a model\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9JBe021yh1s"
   },
   "source": [
    "***\n",
    "__Question 1__:\n",
    "Explain the function act. Why is ```epsilon``` essential?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Des_NiWqyh1u"
   },
   "source": [
    "During training, the function act uses an hyperparameter $\\epsilon$ between 0 and 1 to balance choosing an action by using the policy learned, or randomly. It is used to balance exploitation of the known policy, and exploration of other policies in order to avoid local optimas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNKRIDbRyh1w"
   },
   "source": [
    "***\n",
    "### The Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fNWWcOmyh1z"
   },
   "source": [
    "The ```Agent``` and the ```Environment``` work in an interlaced way as in the following (take some time to understand this code as it is the core of the project)\n",
    "\n",
    "```python\n",
    "\n",
    "epoch = 300\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "\n",
    "# Number of won games\n",
    "score = 0\n",
    "loss = 0\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    # At each epoch, we restart to a fresh game and get the initial state\n",
    "    state = env.reset()\n",
    "    # This assumes that the games will end\n",
    "    game_over = False\n",
    "\n",
    "    win = 0\n",
    "    lose = 0\n",
    "    \n",
    "    while not game_over:\n",
    "        # The agent performs an action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Apply an action to the environment, get the next state, the reward\n",
    "        # and if the games end\n",
    "        prev_state = state\n",
    "        state, reward, game_over = env.act(action)\n",
    "\n",
    "        # Update the counters\n",
    "        if reward > 0:\n",
    "            win = win + reward\n",
    "        if reward < 0:\n",
    "            lose = lose -reward\n",
    "\n",
    "        # Apply the reinforcement strategy\n",
    "        loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "\n",
    "    # Save as a mp4\n",
    "    if e % 10 == 0:\n",
    "        env.draw(e)\n",
    "\n",
    "    # Update stats\n",
    "    score += win-lose\n",
    "\n",
    "    print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "          .format(e, epoch, loss, win, lose, win-lose))\n",
    "    agent.save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmL8ovtIyh11"
   },
   "source": [
    "# The game, *eat cheese*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OY3RaUq7yh12"
   },
   "source": [
    "A rat runs on an island and tries to eat as much as possible. The island is subdivided into $N\\times N$ cells, in which there are cheese (+0.5) and poisonous cells (-1). The rat has a visibility of 2 cells (thus it can see $5^2$ cells). The rat is given a time $T$ to accumulate as much food as possible. It can perform 4 actions: going up, down, left, right. \n",
    "\n",
    "The goal is to code an agent to solve this task that will learn by trial and error. We propose the following environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sD4HCnL2yh15"
   },
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
    "        grid_size = grid_size+4\n",
    "        self.grid_size = grid_size\n",
    "        self.max_time = max_time\n",
    "        self.temperature = temperature\n",
    "\n",
    "        #board on which one plays\n",
    "        self.board = np.zeros((grid_size,grid_size))\n",
    "        self.position = np.zeros((grid_size,grid_size))\n",
    "\n",
    "        # coordinate of the cat\n",
    "        self.x = 0\n",
    "        self.y = 1\n",
    "\n",
    "        # self time\n",
    "        self.t = 0\n",
    "\n",
    "        self.scale=16\n",
    "\n",
    "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "    def draw(self,e):\n",
    "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
    "\n",
    "    def get_frame(self,t):\n",
    "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
    "        b[self.board>0,0] = 256\n",
    "        b[self.board < 0, 2] = 256\n",
    "        b[self.x,self.y,:]=256\n",
    "        b[-2:,:,:]=0\n",
    "        b[:,-2:,:]=0\n",
    "        b[:2,:,:]=0\n",
    "        b[:,:2,:]=0\n",
    "        \n",
    "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        self.to_draw[t,:,:,:]=b\n",
    "\n",
    "\n",
    "    def act(self, action):\n",
    "        \"\"\"This function returns the new state, reward and decides if the\n",
    "        game ends.\"\"\"\n",
    "\n",
    "        self.get_frame(int(self.t))\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[:, -2:] = -1\n",
    "\n",
    "        self.position[self.x, self.y] = 1\n",
    "        if action == 0:\n",
    "            if self.x == self.grid_size-3:\n",
    "                self.x = self.x-1\n",
    "            else:\n",
    "                self.x = self.x + 1\n",
    "        elif action == 1:\n",
    "            if self.x == 2:\n",
    "                self.x = self.x+1\n",
    "            else:\n",
    "                self.x = self.x-1\n",
    "        elif action == 2:\n",
    "            if self.y == self.grid_size - 3:\n",
    "                self.y = self.y - 1\n",
    "            else:\n",
    "                self.y = self.y + 1\n",
    "        elif action == 3:\n",
    "            if self.y == 2:\n",
    "                self.y = self.y + 1\n",
    "            else:\n",
    "                self.y = self.y - 1\n",
    "        else:\n",
    "            RuntimeError('Error: action not recognized')\n",
    "\n",
    "        self.t = self.t + 1\n",
    "        reward = self.board[self.x, self.y]\n",
    "        self.board[self.x, self.y] = 0\n",
    "        game_over = self.t > self.max_time\n",
    "        state = np.concatenate((self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
    "\n",
    "        return state, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
    "\n",
    "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "\n",
    "\n",
    "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
    "\n",
    "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
    "\n",
    "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "        malus[bonus>0]=0\n",
    "\n",
    "        self.board = bonus + malus\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[:,-2:] = -1\n",
    "        self.board[self.x,self.y] = 0\n",
    "        self.t = 0\n",
    "\n",
    "        state = np.concatenate((\n",
    "                               self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "\n",
    "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2YlCusmmyh1_"
   },
   "source": [
    "The following elements are important because they correspond to the hyper parameters for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R5FF4GhOyh2B"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "size = 13\n",
    "T=200\n",
    "temperature=0.3\n",
    "epochs_train=51\n",
    "epochs_test=11\n",
    "\n",
    "# display videos\n",
    "def display_videos(name):\n",
    "    video = io.open(name, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    return '''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rrc8ZJH2yh2J"
   },
   "source": [
    "__Question 2__ Explain the use of the arrays ```position``` and ```board```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrKpi3iyyh2L"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqN-9T6Pyh2M"
   },
   "source": [
    "## Random Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4Rm94_Ryh2P"
   },
   "source": [
    "***\n",
    "__Question 3__ Implement a random Agent (only ```learned_act``` needs to be implemented):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opN-xL_Wyh2Q"
   },
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super(RandomAgent, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def learned_act(self, s):\n",
    "        return np.random.randint(0,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XrIrE2moyh2V"
   },
   "source": [
    "***\n",
    "***\n",
    "__Question 4__ Visualize the game moves. You need to fill in the following function for the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-gQOMZ_yh2W"
   },
   "outputs": [],
   "source": [
    "def test(agent,env,epochs,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "        \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        state = env.reset()\n",
    "        # This assumes that the games will end\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "        \n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose -reward\n",
    "\n",
    "            # Apply the reinforcement strategy\n",
    "            #loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "        \n",
    "        # Save as a mp4\n",
    "        env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score = score + win-lose\n",
    "\n",
    "        print(\"Win/lose count {}/{}. Average score ({})\"\n",
    "              .format(win, lose, score/(1+e)))\n",
    "    print('Final score: '+str(score/epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6682,
     "status": "ok",
     "timestamp": 1579643258053,
     "user": {
      "displayName": "Léo Andéol",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCZ9lXKogam3N-Zp1ZulY_dPre1MZo_DYuijwWz=s64",
      "userId": "15347828844155644240"
     },
     "user_tz": -60
    },
    "id": "0C9TtXggyh2c",
    "outputId": "5a6bcd92-b55c-4739-82e8-3fd513962901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win/lose count 6.5/17.0. Average score (-10.5)\n",
      "Win/lose count 9.5/10.0. Average score (-5.5)\n",
      "Win/lose count 8.0/17.0. Average score (-6.666666666666667)\n",
      "Win/lose count 13.5/21.0. Average score (-6.875)\n",
      "Win/lose count 9.5/11.0. Average score (-5.8)\n",
      "Win/lose count 11.0/11.0. Average score (-4.833333333333333)\n",
      "Win/lose count 7.5/20.0. Average score (-5.928571428571429)\n",
      "Win/lose count 13.0/13.0. Average score (-5.1875)\n",
      "Win/lose count 6.5/9.0. Average score (-4.888888888888889)\n",
      "Win/lose count 11.5/14.0. Average score (-4.65)\n",
      "Win/lose count 9.5/17.0. Average score (-4.909090909090909)\n",
      "Final score: -4.909090909090909\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the game\n",
    "env = Environment(grid_size=size, max_time=T,temperature=temperature)\n",
    "\n",
    "# Initialize the agent!\n",
    "agent = RandomAgent()\n",
    "\n",
    "test(agent,env,epochs_test,prefix='random')\n",
    "HTML(display_videos('random0.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rypa4msqyh2h"
   },
   "source": [
    "***\n",
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6t1I7C1yh2k"
   },
   "source": [
    "Let us assume here that $T=\\infty$.\n",
    "\n",
    "***\n",
    "__Question 5__ Let $\\pi$ be a policy, show that:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{\\pi}(s,a)=E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{\\pi}(s',a')]\n",
    "\\end{equation*}\n",
    "\n",
    "Then, show that for the optimal policy $\\pi^*$ (we assume its existence), the following holds: \n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{*}(s,a)=E_{s'\\sim \\pi^*(.|s,a)}[r(s,a)+\\gamma\\max_{a'}Q^{*}(s',a')].\n",
    "\\end{equation*}\n",
    "Finally, deduce that a plausible objective is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}(\\theta)=E_{s' \\sim \\pi^*(.|s,a)}\\Vert r+\\gamma\\max\\max_{a'}Q(s',a',\\theta)-Q(s,a,\\theta)\\Vert^{2}.\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmLC1O1Hyh2n"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sN4mdK4Gyh2n"
   },
   "source": [
    "***\n",
    "The DQN-learning algorithm relies on these derivations to train the parameters $\\theta$ of a Deep Neural Network:\n",
    "\n",
    "1. At the state $s_t$, select the action $a_t$ with best reward using $Q_t$ and store the results;\n",
    "\n",
    "2. Obtain the new state $s_{t+1}$ from the environment $p$;\n",
    "\n",
    "3. Store $(s_t,a_t,s_{t+1})$;\n",
    "\n",
    "4. Obtain $Q_{t+1}$ by minimizing  $\\mathcal{L}$ from a recovered batch from the previously stored results.\n",
    "\n",
    "***\n",
    "__Question 6__ Implement the class ```Memory``` that stores moves (in a replay buffer) via ```remember``` and provides a ```random_access``` to these. Specify a maximum memory size to avoid side effects. You can for example use a ```list()``` and set by default ```max_memory=100```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u2ObmWp1yh2p"
   },
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, max_memory=100):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "\n",
    "    def remember(self, m):\n",
    "        self.memory.append(m)\n",
    "        if len(self.memory)>self.max_memory:\n",
    "          self.memory = self.memory[1:]\n",
    "\n",
    "    def random_access(self):\n",
    "        return np.random.permutation(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPevv-pYyh2v"
   },
   "source": [
    "***\n",
    "The pipeline we will use for training is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RvoOiDcQyh2x"
   },
   "outputs": [],
   "source": [
    "def train(agent,env,epoch,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "    loss = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will terminate\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose -reward\n",
    "\n",
    "            # Apply the reinforcement strategy\n",
    "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "\n",
    "        # Save as a mp4\n",
    "        if e % 10 == 0:\n",
    "            env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score += win-lose\n",
    "\n",
    "        print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "              .format(e, epoch, loss, win, lose, win-lose))\n",
    "        agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jn7dILVmyh22"
   },
   "source": [
    "***\n",
    "__Question 7__ Implement the DQN training algorithm using a cascade of fully connected layers. You can use different learning rate, batch size or memory size parameters. In particular, the loss might oscillate while the player will start to win the games. You have to find a good criterium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1G7kl54yh24"
   },
   "outputs": [],
   "source": [
    "class DQN(Agent):\n",
    "    def __init__(self, grid_size,  epsilon = 0.1, memory_size=100, batch_size = 16,n_state=2):\n",
    "        super(DQN, self).__init__(epsilon = epsilon)\n",
    "\n",
    "        # Discount for Q learning\n",
    "        self.discount = 0.99\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        # number of state\n",
    "        self.n_state = n_state\n",
    "\n",
    "        # Memory\n",
    "        self.memory = Memory(memory_size)\n",
    "        \n",
    "        # Batch size when learning\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def learned_act(self, s):\n",
    "        #print(\"learned_act\",s.shape)\n",
    "        s2 = self.model.predict(s[np.newaxis,:,:,:])\n",
    "        #print(\"learned_act2\",s.shape)\n",
    "        #print(s2)\n",
    "        a = np.argmax(s2,axis=1)[0]\n",
    "        #print(\"learned_act3\",a)\n",
    "        return a\n",
    "\n",
    "    def reinforce(self, s_, n_s_, a_, r_, game_over_):\n",
    "        # Two steps: first memorize the states, second learn from the pool\n",
    "\n",
    "        self.memory.remember([s_, n_s_, a_, r_, game_over_])\n",
    "        \n",
    "        input_states = np.zeros((self.batch_size, 5,5,self.n_state))\n",
    "        target_q = np.zeros((self.batch_size, 4))\n",
    "\n",
    "        batch = self.memory.random_access()[:self.batch_size]\n",
    "\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            if len(batch)<=i:\n",
    "              break\n",
    "            s_i, n_s_i, a_i, r_i, game_over_i = batch[i]\n",
    "            #print(\"a_i\",a_i)\n",
    "            #print(\"game_\",game_over_i.shape)\n",
    "            #print(\"r_i\",r_i.shape)\n",
    "            input_states[i] = s_i \n",
    "            #default\n",
    "            target_q[i] = self.model.predict(s_i[np.newaxis,:,:,:])\n",
    "            \n",
    "            if game_over_i:\n",
    "                target_q[i,a_i] = r_i\n",
    "            else:\n",
    "                target_q[i,a_i] = r_i + self.discount * np.max(self.model.predict(s_i[np.newaxis,:,:,:]))\n",
    "        #print(\"input_shape\",input_states.shape)\n",
    "        # HINT: Clip the target to avoid exploiding gradients.. -- clipping is a bit tighter\n",
    "        target_q = np.clip(target_q, -3, 3)\n",
    "\n",
    "        l = self.model.train_on_batch(input_states, target_q)\n",
    "\n",
    "\n",
    "        return l\n",
    "\n",
    "    def save(self,name_weights='model.h5',name_model='model.json'):\n",
    "        self.model.save_weights(name_weights, overwrite=True)\n",
    "        with open(name_model, \"w\") as outfile:\n",
    "            json.dump(self.model.to_json(), outfile)\n",
    "            \n",
    "    def load(self,name_weights='model.h5',name_model='model.json'):\n",
    "        with open(name_model, \"r\") as jfile:\n",
    "            model = model_from_json(json.load(jfile))\n",
    "        model.load_weights(name_weights)\n",
    "        model.compile(\"sgd\", \"mse\")\n",
    "        self.model = model\n",
    "\n",
    "            \n",
    "class DQN_FC(DQN):\n",
    "    def __init__(self, *args, lr=0.1,**kwargs):\n",
    "        super(DQN_FC, self).__init__( *args,**kwargs)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(5,5,2)))\n",
    "        model.add(Dense(100))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(30))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(4))\n",
    "        \n",
    "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mse\")\n",
    "        self.model = model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63813,
     "status": "error",
     "timestamp": 1579643321213,
     "user": {
      "displayName": "Léo Andéol",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCZ9lXKogam3N-Zp1ZulY_dPre1MZo_DYuijwWz=s64",
      "userId": "15347828844155644240"
     },
     "user_tz": -60
    },
    "id": "eTbKQvAnyh3A",
    "outputId": "cba7d031-1345-4664-d75e-0d3a8881adb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/051 | Loss 0.0097 | Win/lose count 2.5/2.0 (0.5)\n",
      "Epoch 001/051 | Loss 0.0119 | Win/lose count 1.5/4.0 (-2.5)\n",
      "Epoch 002/051 | Loss 0.0105 | Win/lose count 2.5/1.0 (1.5)\n",
      "Epoch 003/051 | Loss 0.0085 | Win/lose count 3.5/7.0 (-3.5)\n",
      "Epoch 004/051 | Loss 0.0103 | Win/lose count 6.0/4.0 (2.0)\n",
      "Epoch 005/051 | Loss 0.0022 | Win/lose count 3.5/3.0 (0.5)\n",
      "Epoch 006/051 | Loss 0.0026 | Win/lose count 5.0/4.0 (1.0)\n",
      "Epoch 007/051 | Loss 0.0018 | Win/lose count 3.5/3.0 (0.5)\n",
      "Epoch 008/051 | Loss 0.0023 | Win/lose count 2.5/4.0 (-1.5)\n",
      "Epoch 009/051 | Loss 0.0085 | Win/lose count 3.5/2.0 (1.5)\n",
      "Epoch 010/051 | Loss 0.0124 | Win/lose count 4.0/4.0 (0.0)\n",
      "Epoch 011/051 | Loss 0.0075 | Win/lose count 5.0/2.0 (3.0)\n",
      "Epoch 012/051 | Loss 0.0037 | Win/lose count 3.0/2.0 (1.0)\n",
      "Epoch 013/051 | Loss 0.0048 | Win/lose count 4.5/2.0 (2.5)\n",
      "Epoch 014/051 | Loss 0.0012 | Win/lose count 3.5/2.0 (1.5)\n",
      "Epoch 015/051 | Loss 0.0047 | Win/lose count 0.5/1.0 (-0.5)\n",
      "Epoch 016/051 | Loss 0.0020 | Win/lose count 1.5/1.0 (0.5)\n",
      "Epoch 017/051 | Loss 0.0101 | Win/lose count 3.5/2.0 (1.5)\n",
      "Epoch 018/051 | Loss 0.0053 | Win/lose count 2.0/1.0 (1.0)\n",
      "Epoch 019/051 | Loss 0.0269 | Win/lose count 7.5/4.0 (3.5)\n",
      "Epoch 020/051 | Loss 0.0549 | Win/lose count 4.5/5.0 (-0.5)\n",
      "Epoch 021/051 | Loss 0.0010 | Win/lose count 4.5/5.0 (-0.5)\n",
      "Epoch 022/051 | Loss 0.0017 | Win/lose count 1.0/1.0 (0.0)\n",
      "Epoch 023/051 | Loss 0.0027 | Win/lose count 4.0/3.0 (1.0)\n",
      "Epoch 024/051 | Loss 0.0010 | Win/lose count 8.0/7.0 (1.0)\n",
      "Epoch 025/051 | Loss 0.0056 | Win/lose count 4.0/3.0 (1.0)\n",
      "Epoch 026/051 | Loss 0.0012 | Win/lose count 1.5/2.0 (-0.5)\n",
      "Epoch 027/051 | Loss 0.0041 | Win/lose count 4.5/0 (4.5)\n",
      "Epoch 028/051 | Loss 0.0040 | Win/lose count 10.0/2.0 (8.0)\n",
      "Epoch 029/051 | Loss 0.0037 | Win/lose count 7.5/5.0 (2.5)\n",
      "Epoch 030/051 | Loss 0.0050 | Win/lose count 8.0/3.0 (5.0)\n",
      "Epoch 031/051 | Loss 0.0035 | Win/lose count 2.0/2.0 (0.0)\n",
      "Epoch 032/051 | Loss 0.0039 | Win/lose count 7.5/3.0 (4.5)\n",
      "Epoch 033/051 | Loss 0.0051 | Win/lose count 5.0/2.0 (3.0)\n",
      "Epoch 034/051 | Loss 0.0038 | Win/lose count 9.0/6.0 (3.0)\n",
      "Epoch 035/051 | Loss 0.0489 | Win/lose count 4.0/1.0 (3.0)\n",
      "Epoch 036/051 | Loss 0.0011 | Win/lose count 3.0/2.0 (1.0)\n",
      "Epoch 037/051 | Loss 0.0021 | Win/lose count 5.0/5.0 (0.0)\n",
      "Epoch 038/051 | Loss 0.0239 | Win/lose count 5.0/3.0 (2.0)\n",
      "Epoch 039/051 | Loss 0.0033 | Win/lose count 6.0/5.0 (1.0)\n",
      "Epoch 040/051 | Loss 0.0010 | Win/lose count 4.0/0 (4.0)\n",
      "Epoch 041/051 | Loss 0.0030 | Win/lose count 9.5/2.0 (7.5)\n",
      "Epoch 042/051 | Loss 0.0029 | Win/lose count 3.5/0 (3.5)\n",
      "Epoch 043/051 | Loss 0.0075 | Win/lose count 6.0/3.0 (3.0)\n",
      "Epoch 044/051 | Loss 0.0029 | Win/lose count 10.5/5.0 (5.5)\n",
      "Epoch 045/051 | Loss 0.0012 | Win/lose count 12.5/1.0 (11.5)\n",
      "Epoch 046/051 | Loss 0.0165 | Win/lose count 12.5/2.0 (10.5)\n",
      "Epoch 047/051 | Loss 0.0032 | Win/lose count 6.0/2.0 (4.0)\n",
      "Epoch 048/051 | Loss 0.0456 | Win/lose count 12.5/1.0 (11.5)\n",
      "Epoch 049/051 | Loss 0.0041 | Win/lose count 6.0/1.0 (5.0)\n",
      "Epoch 050/051 | Loss 0.0028 | Win/lose count 8.0/2.0 (6.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_FC(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "train(agent, env, epochs_train, prefix='fc_train')\n",
    "HTML(display_videos('fc_train50.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vmgk3JnYyh3E"
   },
   "source": [
    "***\n",
    "***\n",
    "__Question 8__ Implement the DQN training algorithm using a CNN (for example, 2 convolutional layers and one final fully connected layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBbeomzMyh3F"
   },
   "outputs": [],
   "source": [
    "class DQN_CNN(DQN):\n",
    "    def __init__(self, *args,lr=0.1,**kwargs):\n",
    "        super(DQN_CNN, self).__init__(*args,**kwargs)\n",
    "        \n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=30,kernel_size=3,input_shape=(5,5,self.n_state)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(filters=20,kernel_size=3))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4))\n",
    "        \n",
    "        \n",
    "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mse\")\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 377538,
     "status": "error",
     "timestamp": 1579645042154,
     "user": {
      "displayName": "Léo Andéol",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCZ9lXKogam3N-Zp1ZulY_dPre1MZo_DYuijwWz=s64",
      "userId": "15347828844155644240"
     },
     "user_tz": -60
    },
    "id": "zibomlQgyh3N",
    "outputId": "f695ec61-c1af-4fa6-8946-9948927e334a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/051 | Loss 0.0099 | Win/lose count 7.0/5.0 (2.0)\n",
      "Epoch 001/051 | Loss 0.0068 | Win/lose count 6.5/5.0 (1.5)\n",
      "Epoch 002/051 | Loss 0.0044 | Win/lose count 1.5/7.0 (-5.5)\n",
      "Epoch 003/051 | Loss 0.0001 | Win/lose count 4.0/8.0 (-4.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-064547fb99a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cnn_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cnn_train50.mp4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-ebd24fe4447b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, env, epoch, prefix)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# Apply the reinforcement strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Save as a mp4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-477223955f1c>\u001b[0m in \u001b[0;36mreinforce\u001b[0;34m(self, s_, n_s_, a_, r_, game_over_)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mtarget_q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mtarget_q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;31m#print(\"input_shape\",input_states.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# HINT: Clip the target to avoid exploiding gradients.. -- clipping is a bit tighter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3486\u001b[0m     \u001b[0;31m# this ensures that we return its value as a SparseTensorValue rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3487\u001b[0m     \u001b[0;31m# a SparseTensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3488\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_if_composite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m                           expand_composites=expand_composites)\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m   \u001b[0mflat_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m   \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    530\u001b[0m                           expand_composites=expand_composites)\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m   \u001b[0mflat_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m   \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mnest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m   \"\"\"\n\u001b[0;32m--> 263\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mFlatten\u001b[0;34m(nested, expand_composites)\u001b[0m\n\u001b[1;32m   2649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m     \"\"\"\n\u001b[0;32m-> 2651\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mIsSequenceForData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(size, lr=.01, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "train(agent,env,epochs_train,prefix='cnn_train')\n",
    "HTML(display_videos('cnn_train50.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2eujJaMGyh3Q"
   },
   "source": [
    "***\n",
    "***\n",
    "__Question 9__ Test both algorithms and compare their performances. Which issue(s) do you observe? Observe also different behaviors by changing the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13rs9poPyh3Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of the CNN\n",
      "Win/lose count 5.0/0. Average score (5.0)\n",
      "Win/lose count 11.0/4.0. Average score (6.0)\n",
      "Win/lose count 8.0/1.0. Average score (6.333333333333333)\n",
      "Win/lose count 15.0/8.0. Average score (6.5)\n",
      "Win/lose count 12.0/3.0. Average score (7.0)\n",
      "Win/lose count 16.0/3.0. Average score (8.0)\n",
      "Win/lose count 4.0/1.0. Average score (7.285714285714286)\n",
      "Win/lose count 7.0/0. Average score (7.25)\n",
      "Win/lose count 5.0/3.0. Average score (6.666666666666667)\n",
      "Win/lose count 5.0/1.0. Average score (6.4)\n",
      "Win/lose count 14.0/3.0. Average score (6.818181818181818)\n",
      "Final score: 6.818181818181818\n",
      "Test of the FC\n",
      "Win/lose count 1.5/4.0. Average score (-2.5)\n",
      "Win/lose count 5.0/4.0. Average score (-0.75)\n",
      "Win/lose count 3.0/6.0. Average score (-1.5)\n",
      "Win/lose count 3.5/3.0. Average score (-1.0)\n",
      "Win/lose count 5.0/5.0. Average score (-0.8)\n",
      "Win/lose count 0.5/3.0. Average score (-1.0833333333333333)\n",
      "Win/lose count 3.0/2.0. Average score (-0.7857142857142857)\n",
      "Win/lose count 1.0/4.0. Average score (-1.0625)\n",
      "Win/lose count 1.0/6.0. Average score (-1.5)\n",
      "Win/lose count 2.5/1.0. Average score (-1.2)\n",
      "Win/lose count 5.5/7.0. Average score (-1.2272727272727273)\n",
      "Final score: -1.2272727272727273\n"
     ]
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T,temperature=0.3)\n",
    "agent_cnn = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "agent_cnn.load(name_weights='cnn_trainmodel.h5',name_model='cnn_trainmodel.json')\n",
    "\n",
    "agent_fc = DQN_FC(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "agent_cnn.load(name_weights='fc_trainmodel.h5',name_model='fc_trainmodel.json')\n",
    "print('Test of the CNN')\n",
    "test(agent_cnn,env,epochs_test,prefix='cnn_test')\n",
    "print('Test of the FC')\n",
    "test(agent_fc,env,epochs_test,prefix='fc_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAikup5fyh3U"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(display_videos('cnn_test10.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0k8CSazyh3X"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(display_videos('fc_test10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgyaBG4Fyh3e"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T6wCcXjXyh3f"
   },
   "source": [
    "***\n",
    "\n",
    "The algorithm tends to not explore the map which can be an issue. We propose two ideas in order to encourage exploration:\n",
    "1. Incorporating a decreasing $\\epsilon$-greedy exploration. You can use the method ```set_epsilon```\n",
    "2. Append via the environment a new state that describes if a cell has been visited or not\n",
    "\n",
    "***\n",
    "__Question 10__ Design a new ```train_explore``` function and environment class ```EnvironmentExploring``` to tackle the issue of exploration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MmW4dpUIyh3h"
   },
   "outputs": [],
   "source": [
    "def train_explore(agent,env,epoch,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "    loss = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will terminate\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        agent.set_epsilon(agent.epsilon*0.9)\n",
    "        print(\"New Epsilon = \",agent.epsilon)\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose -reward\n",
    "\n",
    "            # Apply the reinforcement strategy\n",
    "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "\n",
    "        # Save as a mp4\n",
    "        if e % 10 == 0:\n",
    "            env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score += win-lose\n",
    "\n",
    "        print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "              .format(e, epoch, loss, win, lose, win-lose))\n",
    "        agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')\n",
    "        \n",
    "class EnvironmentExploring(object):\n",
    "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
    "        grid_size = grid_size+4\n",
    "        self.grid_size = grid_size\n",
    "        self.max_time = max_time\n",
    "        self.temperature = temperature\n",
    "\n",
    "        #board on which one plays\n",
    "        self.board = np.zeros((grid_size,grid_size))\n",
    "        self.position = np.zeros((grid_size,grid_size))\n",
    "        self.malus_position = np.zeros((grid_size,grid_size))\n",
    "\n",
    "        # coordinate of the cat\n",
    "        self.x = 0\n",
    "        self.y = 1\n",
    "\n",
    "        # self time\n",
    "        self.t = 0\n",
    "\n",
    "        self.scale=16\n",
    "\n",
    "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "    def draw(self,e):\n",
    "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
    "\n",
    "    def get_frame(self,t):\n",
    "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
    "        b[self.board>0,0] = 256\n",
    "        b[self.board < 0, 2] = 256\n",
    "        b[self.x,self.y,:]=256\n",
    "        b[-2:,:,:]=0\n",
    "        b[:,-2:,:]=0\n",
    "        b[:2,:,:]=0\n",
    "        b[:,:2,:]=0\n",
    "        \n",
    "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        self.to_draw[t,:,:,:]=b\n",
    "\n",
    "\n",
    "    def act(self, action):\n",
    "        \"\"\"This function returns the new state, reward and decides if the\n",
    "        game ends.\"\"\"\n",
    "\n",
    "        self.get_frame(int(self.t))\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[:,-2:] = -1\n",
    "\n",
    "        self.position[self.x, self.y] = 1\n",
    "        if action == 0:\n",
    "            if self.x == self.grid_size-3:\n",
    "                self.x = self.x-1\n",
    "            else:\n",
    "                self.x = self.x + 1\n",
    "        elif action == 1:\n",
    "            if self.x == 2:\n",
    "                self.x = self.x+1\n",
    "            else:\n",
    "                self.x = self.x-1\n",
    "        elif action == 2:\n",
    "            if self.y == self.grid_size - 3:\n",
    "                self.y = self.y - 1\n",
    "            else:\n",
    "                self.y = self.y + 1\n",
    "        elif action == 3:\n",
    "            if self.y == 2:\n",
    "                self.y = self.y + 1\n",
    "            else:\n",
    "                self.y = self.y - 1\n",
    "        else:\n",
    "            RuntimeError('Error: action not recognized')\n",
    "\n",
    "        self.t = self.t + 1\n",
    "        #reward = self.board[self.x, self.y]\n",
    "        game_over = self.t > self.max_time\n",
    "        reward = 0\n",
    "        if train:\n",
    "            reward = -self.malus_position[self.x, self.y]\n",
    "        self.malus_position[self.x, self.y] = 0.1\n",
    "\n",
    "        reward = reward + self.board[self.x, self.y]\n",
    "        self.board[self.x, self.y] = 0\n",
    "        # 3 \"feature\" states instead of 2\n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                        self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
    "\n",
    "        return state, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
    "\n",
    "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "\n",
    "\n",
    "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
    "\n",
    "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
    "\n",
    "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "        malus[bonus>0]=0\n",
    "\n",
    "        self.board = bonus + malus\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[:,-2:] = -1\n",
    "        self.board[self.x,self.y] = 0\n",
    "        self.t = 0\n",
    "        #reset also the malus\n",
    "        self.malus_position = np.zeros((self.grid_size,self.grid_size))\n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                        self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "\n",
    "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
    "        return state\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XeMqtF3cyh3l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Epsilon =  0.81\n",
      "Epoch 000/051 | Loss 0.0227 | Win/lose count 7.5/32.400000000000084 (-24.900000000000084)\n",
      "New Epsilon =  0.7290000000000001\n",
      "Epoch 001/051 | Loss 0.0182 | Win/lose count 11.5/26.50000000000007 (-15.000000000000071)\n",
      "New Epsilon =  0.6561000000000001\n",
      "Epoch 002/051 | Loss 0.0157 | Win/lose count 14.0/21.200000000000045 (-7.2000000000000455)\n",
      "New Epsilon =  0.5904900000000002\n",
      "Epoch 003/051 | Loss 0.0098 | Win/lose count 9.0/21.900000000000034 (-12.900000000000034)\n",
      "New Epsilon =  0.5314410000000002\n",
      "Epoch 004/051 | Loss 0.0099 | Win/lose count 20.0/22.20000000000005 (-2.200000000000049)\n",
      "New Epsilon =  0.47829690000000014\n",
      "Epoch 005/051 | Loss 0.0086 | Win/lose count 10.5/21.40000000000003 (-10.90000000000003)\n",
      "New Epsilon =  0.43046721000000016\n",
      "Epoch 006/051 | Loss 0.0063 | Win/lose count 13.5/21.900000000000027 (-8.400000000000027)\n",
      "New Epsilon =  0.38742048900000015\n",
      "Epoch 007/051 | Loss 0.0055 | Win/lose count 15.0/21.40000000000002 (-6.40000000000002)\n",
      "New Epsilon =  0.34867844010000015\n",
      "Epoch 008/051 | Loss 0.0303 | Win/lose count 17.5/22.40000000000005 (-4.900000000000048)\n",
      "New Epsilon =  0.31381059609000017\n",
      "Epoch 009/051 | Loss 0.0059 | Win/lose count 12.0/20.0 (-8.0)\n",
      "New Epsilon =  0.28242953648100017\n",
      "Epoch 010/051 | Loss 0.0094 | Win/lose count 12.0/24.40000000000007 (-12.40000000000007)\n",
      "New Epsilon =  0.25418658283290013\n",
      "Epoch 011/051 | Loss 0.0136 | Win/lose count 6.5/17.39999999999998 (-10.89999999999998)\n",
      "New Epsilon =  0.22876792454961012\n",
      "Epoch 012/051 | Loss 0.0095 | Win/lose count 7.5/21.800000000000033 (-14.300000000000033)\n",
      "New Epsilon =  0.2058911320946491\n",
      "Epoch 013/051 | Loss 0.0123 | Win/lose count 15.5/19.000000000000018 (-3.5000000000000178)\n",
      "New Epsilon =  0.1853020188851842\n",
      "Epoch 014/051 | Loss 0.0063 | Win/lose count 17.5/21.30000000000002 (-3.8000000000000185)\n",
      "New Epsilon =  0.16677181699666577\n",
      "Epoch 015/051 | Loss 0.0029 | Win/lose count 17.0/18.89999999999999 (-1.8999999999999915)\n",
      "New Epsilon =  0.1500946352969992\n",
      "Epoch 016/051 | Loss 0.0143 | Win/lose count 22.0/16.19999999999997 (5.800000000000029)\n",
      "New Epsilon =  0.13508517176729928\n",
      "Epoch 017/051 | Loss 0.0113 | Win/lose count 22.0/14.099999999999977 (7.9000000000000234)\n",
      "New Epsilon =  0.12157665459056936\n",
      "Epoch 018/051 | Loss 0.0084 | Win/lose count 12.0/20.800000000000004 (-8.800000000000004)\n",
      "New Epsilon =  0.10941898913151243\n",
      "Epoch 019/051 | Loss 0.0225 | Win/lose count 18.5/13.99999999999997 (4.50000000000003)\n",
      "New Epsilon =  0.0984770902183612\n",
      "Epoch 020/051 | Loss 0.0031 | Win/lose count 19.5/17.399999999999995 (2.100000000000005)\n",
      "New Epsilon =  0.08862938119652508\n",
      "Epoch 021/051 | Loss 0.0072 | Win/lose count 11.5/19.3 (-7.800000000000001)\n",
      "New Epsilon =  0.07976644307687257\n",
      "Epoch 022/051 | Loss 0.0035 | Win/lose count 21.0/11.399999999999975 (9.600000000000025)\n",
      "New Epsilon =  0.07178979876918531\n",
      "Epoch 023/051 | Loss 0.0081 | Win/lose count 24.0/12.899999999999974 (11.100000000000026)\n",
      "New Epsilon =  0.06461081889226679\n",
      "Epoch 024/051 | Loss 0.0079 | Win/lose count 28.5/13.09999999999998 (15.40000000000002)\n",
      "New Epsilon =  0.05814973700304011\n",
      "Epoch 025/051 | Loss 0.0063 | Win/lose count 21.5/16.199999999999974 (5.300000000000026)\n",
      "New Epsilon =  0.0523347633027361\n",
      "Epoch 026/051 | Loss 0.0092 | Win/lose count 20.5/15.399999999999965 (5.100000000000035)\n",
      "New Epsilon =  0.04710128697246249\n",
      "Epoch 027/051 | Loss 0.0119 | Win/lose count 20.5/11.899999999999977 (8.600000000000023)\n",
      "New Epsilon =  0.042391158275216244\n",
      "Epoch 028/051 | Loss 0.0045 | Win/lose count 20.0/13.099999999999973 (6.900000000000027)\n",
      "New Epsilon =  0.03815204244769462\n",
      "Epoch 029/051 | Loss 0.0036 | Win/lose count 18.5/12.599999999999971 (5.900000000000029)\n",
      "New Epsilon =  0.03433683820292516\n",
      "Epoch 030/051 | Loss 0.0069 | Win/lose count 11.0/17.799999999999986 (-6.7999999999999865)\n",
      "New Epsilon =  0.030903154382632643\n",
      "Epoch 031/051 | Loss 0.0121 | Win/lose count 9.0/16.999999999999975 (-7.999999999999975)\n",
      "New Epsilon =  0.02781283894436938\n",
      "Epoch 032/051 | Loss 0.0066 | Win/lose count 17.0/14.29999999999997 (2.7000000000000295)\n",
      "New Epsilon =  0.025031555049932444\n",
      "Epoch 033/051 | Loss 0.0089 | Win/lose count 15.0/13.299999999999969 (1.7000000000000313)\n",
      "New Epsilon =  0.0225283995449392\n",
      "Epoch 034/051 | Loss 0.0083 | Win/lose count 15.5/15.499999999999964 (3.552713678800501e-14)\n",
      "New Epsilon =  0.020275559590445278\n",
      "Epoch 035/051 | Loss 0.0040 | Win/lose count 20.0/13.599999999999975 (6.400000000000025)\n",
      "New Epsilon =  0.01824800363140075\n",
      "Epoch 036/051 | Loss 0.0148 | Win/lose count 14.0/13.99999999999997 (3.019806626980426e-14)\n",
      "New Epsilon =  0.016423203268260675\n",
      "Epoch 037/051 | Loss 0.0098 | Win/lose count 13.5/14.299999999999967 (-0.799999999999967)\n",
      "New Epsilon =  0.014780882941434608\n",
      "Epoch 038/051 | Loss 0.0047 | Win/lose count 20.0/11.499999999999979 (8.500000000000021)\n",
      "New Epsilon =  0.013302794647291147\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "env = EnvironmentExploring(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(size, lr=.1, epsilon = 0.9, memory_size=2000, batch_size = 32,n_state=3)\n",
    "train_explore(agent, env, epochs_train, prefix='cnn_train_explore')\n",
    "HTML(display_videos('cnn_train_explore50.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NfkkNku9yh3o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win/lose count 0/17.999999999999986. Average score (-17.999999999999986)\n",
      "Win/lose count 0/17.999999999999986. Average score (-17.999999999999986)\n",
      "Win/lose count 0/17.69999999999998. Average score (-17.899999999999984)\n",
      "Win/lose count 0/15.89999999999996. Average score (-17.399999999999977)\n",
      "Win/lose count 0/15.69999999999996. Average score (-17.059999999999974)\n",
      "Win/lose count 0/16.599999999999966. Average score (-16.983333333333306)\n",
      "Win/lose count 0/18.29999999999999. Average score (-17.171428571428546)\n",
      "Win/lose count 0/17.099999999999973. Average score (-17.162499999999973)\n",
      "Win/lose count 0/16.699999999999967. Average score (-17.111111111111082)\n",
      "Win/lose count 0/18.19999999999999. Average score (-17.219999999999974)\n",
      "Win/lose count 0/16.89999999999997. Average score (-17.190909090909063)\n",
      "Final score: -17.190909090909063\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAFtZtZGF0AAACrQYF//+p3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1NSByMjkxNyAwYTg0ZDk4IC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAxOCAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MToweDExMSBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MCBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD00IHRocmVhZHM9NiBsb29rYWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFjZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJhbWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdlaWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MjUgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVzaD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBxcG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAKsZYiEADf//vaH+BTZWBP+Wb/9DX/cj9uPrP1xYyEE31qvIejAGS+1H+b/rFFs6Z6UB/fgCJQAc24ZwpHZJ46i9VkGOD4FNXSdXwKShPP6X05I5769RFTBRlJkiDu2YfsuqtEBFQA+m2u5kJZ3GQU/Ngb9Z3dDSSACQ0vNI3pgeiN+RIHVdx219wcECAa7nR/qEdh8RYyH1fVovUWAQmR0/MLyPC4hUM8ZLUzOCfdPgwYVeOV2qYD1E+SsILIAM6e8gzzm0f6zfEKvbODpWko04PEMCsAI54MJZtgPoqF0Mz3fhE150YuIPL1t89WtOrjbgthSX7dpH16SC663fWFzZU4XdJBdFWIFo5cIe1IqpEJOm4K78zGhvCm8lMs5zbWBuSj4hg1VSJ8AUSqjk4UAxF7qdCIrixq+sBp8ldcDzRZArQKWz86039wQEeYPk+iEew1YFT2lUuJA0comp2BT21haHeBWRjDvgnN4kCbXGOY+aHJi2bsIYaWiEV3gZ83lwiC0UvyrPodlEgLhd+tpLl/K6ZzUY9laiJzulfieLAIipWcLItM+8NzhzISal8WC8SpnhfiMx0IsHhj23UtXr9UWlQbTSU13IkLqcdz82JYnfoD7wF+vOhS/o92QDEYAZC6LeAIOGHXayyfL5ETJwVvzjEQDcBal75Sif01PLHSXJ0u9SfWbPdVhQWFgCV8Wjilg16/8Uu9g96yUNPU+xt75I9aV21LQz4jj/lX6EpE4RHzRRD/7gLrljH6wp53z9I6rQiF5Qa0Np3MLhFOya5GZpNYHgE3kgCWwjFGTWNqPTDQLNWQsTAYRdg2JzHBdzYoCg6YUbUilS2+fUc0s3Aa1WyaF9ZnaNxFSBS9EP5Y5vNEoOKkSEWkifB5EbA+MOFPGTp9inYZgAEHBAAAAFkGaI2xDP/6eEABu/X39SM//xsVOlKQAAAAbQZ5BeIV/ABecybmWUSgjmV6LTmWPVtzddvT3AAAADwGeYmpCvwAX4lpUigSsLgAAABpBmmRJqEFomUwIb//+p4QAEm+On1HGhIdbQQAAAB9BmoZJ4QpSZTBREsN//qeEABLV2K3RbQGA5vqsPrdLAAAAEAGepWpCvwAPMzwh40NZYYEAAAAYQZqnSeEOiZTAhv/+p4QAHPOM/1KQCtPBAAAAK0Gay0nhDyZTAhv//qeEADE+y+r4FNfUK/ApUtn4FM7A57X/4tgyq+mVpsAAAAAVQZ7pRRE8L/8AHQT1jA9M4tTRjCIwAAAADwGfCHRCvwAlvpO4NkvIBwAAABABnwpqQr8AJ9YR5LmfJWaAAAAAHUGbDUmoQWiZTBTwz/6eEADE+vv1QjlPOXxFXRrYAAAAEAGfLGpCvwAo9KN5piradMEAAAAYQZsuSeEKUmUwIZ/+nhAAfr1xt7033XBnAAAAGUGbT0nhDomUwIb//qeEACDfRz7mRQkOZUEAAAAYQZtwSeEPJlMCG//+p4QAFa91OP8Pq26jAAAAGUGbkUnhDyZTAh3//qmWAAqnvq+uxBuKmTAAAAAoQZu1SeEPJlMCHf/+qZYABvvaX7PN3z4dtXmWWMKM8Cma4fuBRJzoEQAAABVBn9NFETwv/wAILPWpy30WM5+pHGQAAAAQAZ/ydEK/AAtdo7ytlD2SwAAAABABn/RqQr8AB21cGuPFW3UhAAAAH0Gb+UmoQWiZTAh3//6plgADGe2ofP32v1xWYtNxrb4AAAAQQZ4XRREsL/8AA5/3ndXj6wAAAA8BnjZ0Qr8ABPrR3nnGmYEAAAAQAZ44akK/AATWWQw+gJB96AAAABpBmjtJqEFsmUwUTDv//qmWAAMBc6R/fV94uwAAABABnlpqQr8ABPlGiZE0rT/AAAAAGEGaX0nhClJlMCHf/qmWAALx76vuX5uhlQAAABNBnn1FNEwv/wADixIZoGj9ciWNAAAAEAGenHRCvwAE1dWjJLf7W4AAAAAQAZ6eakK/AATXZ5bhs2tbgAAAABNBmoNJqEFomUwId//+qZYAAJWBAAAAEEGeoUURLC//AAOLEtm/SZsAAAAQAZ7AdEK/AATV1aMkt/tbgQAAABABnsJqQr8ABNdnluGza1uAAAAAE0Gax0moQWyZTAh3//6plgAAlYEAAAAQQZ7lRRUsL/8AA4sS2b9JmwAAABABnwR0Qr8ABNXVoyS3+1uBAAAAEAGfBmpCvwAE12eW4bNrW4EAAAATQZsLSahBbJlMCHf//qmWAACVgAAAABBBnylFFSwv/wADixLZv0mbAAAAEAGfSHRCvwAE1dWjJLf7W4EAAAAQAZ9KakK/AATXZ5bhs2tbgAAAABNBm09JqEFsmUwId//+qZYAAJWAAAAAEEGfbUUVLC//AAOLEtm/SZsAAAAQAZ+MdEK/AATV1aMkt/tbgQAAABABn45qQr8ABNdnluGza1uBAAAAE0Gbk0moQWyZTAh3//6plgAAlYAAAAAQQZ+xRRUsL/8AA4sS2b9JmwAAABABn9B0Qr8ABNXVoyS3+1uBAAAAEAGf0mpCvwAE12eW4bNrW4AAAAATQZvXSahBbJlMCHf//qmWAACVgAAAABBBn/VFFSwv/wADixLZv0mbAAAAEAGeFHRCvwAE1dWjJLf7W4AAAAAQAZ4WakK/AATXZ5bhs2tbgQAAABNBmhtJqEFsmUwId//+qZYAAJWBAAAAEEGeOUUVLC//AAOLEtm/SZsAAAAQAZ5YdEK/AATV1aMkt/tbgQAAABABnlpqQr8ABNdnluGza1uAAAAAE0GaX0moQWyZTAh3//6plgAAlYEAAAAQQZ59RRUsL/8AA4sS2b9JmwAAABABnpx0Qr8ABNXVoyS3+1uAAAAAEAGenmpCvwAE12eW4bNrW4AAAAATQZqDSahBbJlMCHf//qmWAACVgQAAABBBnqFFFSwv/wADixLZv0mbAAAAEAGewHRCvwAE1dWjJLf7W4EAAAAQAZ7CakK/AATXZ5bhs2tbgAAAABNBmsdJqEFsmUwId//+qZYAAJWBAAAAEEGe5UUVLC//AAOLEtm/SZsAAAAQAZ8EdEK/AATV1aMkt/tbgQAAABABnwZqQr8ABNdnluGza1uBAAAAE0GbC0moQWyZTAh3//6plgAAlYAAAAAQQZ8pRRUsL/8AA4sS2b9JmwAAABABn0h0Qr8ABNXVoyS3+1uBAAAAEAGfSmpCvwAE12eW4bNrW4AAAAATQZtPSahBbJlMCHf//qmWAACVgAAAABBBn21FFSwv/wADixLZv0mbAAAAEAGfjHRCvwAE1dWjJLf7W4EAAAAQAZ+OakK/AATXZ5bhs2tbgQAAABNBm5NJqEFsmUwId//+qZYAAJWAAAAAEEGfsUUVLC//AAOLEtm/SZsAAAAQAZ/QdEK/AATV1aMkt/tbgQAAABABn9JqQr8ABNdnluGza1uAAAAAE0Gb10moQWyZTAh3//6plgAAlYAAAAAQQZ/1RRUsL/8AA4sS2b9JmwAAABABnhR0Qr8ABNXVoyS3+1uAAAAAEAGeFmpCvwAE12eW4bNrW4EAAAATQZobSahBbJlMCHf//qmWAACVgQAAABBBnjlFFSwv/wADixLZv0mbAAAAEAGeWHRCvwAE1dWjJLf7W4EAAAAQAZ5aakK/AATXZ5bhs2tbgAAAABNBml9JqEFsmUwId//+qZYAAJWBAAAAEEGefUUVLC//AAOLEtm/SZsAAAAQAZ6cdEK/AATV1aMkt/tbgAAAABABnp5qQr8ABNdnluGza1uAAAAAE0Gag0moQWyZTAh3//6plgAAlYEAAAAQQZ6hRRUsL/8AA4sS2b9JmwAAABABnsB0Qr8ABNXVoyS3+1uBAAAAEAGewmpCvwAE12eW4bNrW4AAAAATQZrHSahBbJlMCHf//qmWAACVgQAAABBBnuVFFSwv/wADixLZv0mbAAAAEAGfBHRCvwAE1dWjJLf7W4EAAAAQAZ8GakK/AATXZ5bhs2tbgQAAABNBmwtJqEFsmUwId//+qZYAAJWAAAAAEEGfKUUVLC//AAOLEtm/SZsAAAAQAZ9IdEK/AATV1aMkt/tbgQAAABABn0pqQr8ABNdnluGza1uAAAAAE0GbT0moQWyZTAh3//6plgAAlYAAAAAQQZ9tRRUsL/8AA4sS2b9JmwAAABABn4x0Qr8ABNXVoyS3+1uBAAAAEAGfjmpCvwAE12eW4bNrW4EAAAATQZuTSahBbJlMCHf//qmWAACVgAAAABBBn7FFFSwv/wADixLZv0mbAAAAEAGf0HRCvwAE1dWjJLf7W4EAAAAQAZ/SakK/AATXZ5bhs2tbgAAAABNBm9dJqEFsmUwId//+qZYAAJWAAAAAEEGf9UUVLC//AAOLEtm/SZsAAAAQAZ4UdEK/AATV1aMkt/tbgAAAABABnhZqQr8ABNdnluGza1uBAAAAE0GaG0moQWyZTAh3//6plgAAlYEAAAAQQZ45RRUsL/8AA4sS2b9JmwAAABABnlh0Qr8ABNXVoyS3+1uBAAAAEAGeWmpCvwAE12eW4bNrW4AAAAATQZpfSahBbJlMCHf//qmWAACVgQAAABBBnn1FFSwv/wADixLZv0mbAAAAEAGenHRCvwAE1dWjJLf7W4AAAAAQAZ6eakK/AATXZ5bhs2tbgAAAABNBmoNJqEFsmUwId//+qZYAAJWBAAAAEEGeoUUVLC//AAOLEtm/SZsAAAAQAZ7AdEK/AATV1aMkt/tbgQAAABABnsJqQr8ABNdnluGza1uAAAAAE0Gax0moQWyZTAh3//6plgAAlYEAAAAQQZ7lRRUsL/8AA4sS2b9JmwAAABABnwR0Qr8ABNXVoyS3+1uBAAAAEAGfBmpCvwAE12eW4bNrW4EAAAATQZsLSahBbJlMCHf//qmWAACVgAAAABBBnylFFSwv/wADixLZv0mbAAAAEAGfSHRCvwAE1dWjJLf7W4EAAAAQAZ9KakK/AATXZ5bhs2tbgAAAABNBm09JqEFsmUwId//+qZYAAJWAAAAAEEGfbUUVLC//AAOLEtm/SZsAAAAQAZ+MdEK/AATV1aMkt/tbgQAAABABn45qQr8ABNdnluGza1uBAAAAE0Gbk0moQWyZTAh3//6plgAAlYAAAAAQQZ+xRRUsL/8AA4sS2b9JmwAAABABn9B0Qr8ABNXVoyS3+1uBAAAAEAGf0mpCvwAE12eW4bNrW4AAAAATQZvXSahBbJlMCHf//qmWAACVgAAAABBBn/VFFSwv/wADixLZv0mbAAAAEAGeFHRCvwAE1dWjJLf7W4AAAAAQAZ4WakK/AATXZ5bhs2tbgQAAABNBmhtJqEFsmUwId//+qZYAAJWBAAAAEEGeOUUVLC//AAOLEtm/SZsAAAAQAZ5YdEK/AATV1aMkt/tbgQAAABABnlpqQr8ABNdnluGza1uAAAAAE0GaX0moQWyZTAh3//6plgAAlYEAAAAQQZ59RRUsL/8AA4sS2b9JmwAAABABnpx0Qr8ABNXVoyS3+1uAAAAAEAGenmpCvwAE12eW4bNrW4AAAAATQZqDSahBbJlMCHf//qmWAACVgQAAABBBnqFFFSwv/wADixLZv0mbAAAAEAGewHRCvwAE1dWjJLf7W4EAAAAQAZ7CakK/AATXZ5bhs2tbgAAAABNBmsdJqEFsmUwId//+qZYAAJWBAAAAEEGe5UUVLC//AAOLEtm/SZsAAAAQAZ8EdEK/AATV1aMkt/tbgQAAABABnwZqQr8ABNdnluGza1uBAAAAE0GbC0moQWyZTAh3//6plgAAlYAAAAAQQZ8pRRUsL/8AA4sS2b9JmwAAABABn0h0Qr8ABNXVoyS3+1uBAAAAEAGfSmpCvwAE12eW4bNrW4AAAAATQZtPSahBbJlMCHf//qmWAACVgAAAABBBn21FFSwv/wADixLZv0mbAAAAEAGfjHRCvwAE1dWjJLf7W4EAAAAQAZ+OakK/AATXZ5bhs2tbgQAAABNBm5NJqEFsmUwId//+qZYAAJWAAAAAEEGfsUUVLC//AAOLEtm/SZsAAAAQAZ/QdEK/AATV1aMkt/tbgQAAABABn9JqQr8ABNdnluGza1uAAAAAEkGb10moQWyZTAhv//6nhAABJwAAABBBn/VFFSwv/wADixLZv0mbAAAAEAGeFHRCvwAE1dWjJLf7W4AAAAAQAZ4WakK/AATXZ5bhs2tbgQAAABlBmhpJqEFsmUwIZ//+nhAAF1902MuTZV/9AAAAD0GeOEUVLCv/AATWTcPCQAAAAA8BnllqQr8ABLg1gXX+UUEAAAAaQZpbSahBbJlMCG///qeEAAjqALNts+z570AAAAAeQZp9SeEKUmUwUVLDf/6nhAAI98dPd5uvvrZihIAPAAAAEAGenGpCvwAHQBec60MMDcEAAAAYQZqeSeEOiZTAhv/+p4QABbPdTj/D6txzAAAAGkGaoUnhDyZTAhv//qeEAAWsE/4n+WyUubWAAAAAEkGe30URPCv/AASXYr2Fgv1ygQAAAA8BnuBqQr8ABJdnluGza2sAAAAcQZrjSahBaJlMFPDP/p4QABY/dN9r3nKtxVn1oQAAABABnwJqQr8ABLc0bzTFW5DAAAAAHEGbBUnhClJlMFLDP/6eEAAXOvc1xz+bX199zVEAAAAQAZ8kakK/AATXZ5bhs2tbgQAAABhBmyZJ4Q6JlMCGf/6eEAAjpwjn8Oc32GMAAAAaQZtJS+EIQ8kRggoB/IB/YeAIV//+OEAAEXEAAAAnQZ9nRRE8K/8Cr2PtQcTdqsNJJuWqhgcstbvNKiCjU2XoJ7zXCjzgAAAAJgGfiGpCvwKvY+1BxN2qw0km5aqGByy1u80qJCYAd3gf6sgHD7XsAAAMUG1vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAAB+QAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAt6dHJhawAAAFx0a2hkAAAAAwAAAAAAAAAAAAAAAQAAAAAAAB+QAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAEQAAABEAAAAAAAJGVkdHMAAAAcZWxzdAAAAAAAAAABAAAfkAAABAAAAQAAAAAK8m1kaWEAAAAgbWRoZAAAAAAAAAAAAAAAAAAAMgAAAZQAVcQAAAAAAC1oZGxyAAAAAAAAAAB2aWRlAAAAAAAAAAAAAAAAVmlkZW9IYW5kbGVyAAAACp1taW5mAAAAFHZtaGQAAAABAAAAAAAAAAAAAAAkZGluZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAAAAEAAApdc3RibAAAAJVzdHNkAAAAAAAAAAEAAACFYXZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAEQARAASAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAAAC9hdmNDAfQADf/hABdn9AANkZsoIhHQgAAAAwCAAAAZB4oUywEABWjr48RIAAAAGHN0dHMAAAAAAAAAAQAAAMoAAAIAAAAAFHN0c3MAAAAAAAAAAQAAAAEAAAYoY3R0cwAAAAAAAADDAAAAAQAABAAAAAABAAAIAAAAAAIAAAIAAAAAAQAABAAAAAABAAAGAAAAAAEAAAIAAAAAAQAABAAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAAEAAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAEAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAQAAAAAAQAACAAAAAACAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAABAAAAAABAAAIAAAAAAIAAAIAAAAAHHN0c2MAAAAAAAAAAQAAAAEAAADKAAAAAQAAAzxzdHN6AAAAAAAAAAAAAADKAAAFYQAAABoAAAAfAAAAEwAAAB4AAAAjAAAAFAAAABwAAAAvAAAAGQAAABMAAAAUAAAAIQAAABQAAAAcAAAAHQAAABwAAAAdAAAALAAAABkAAAAUAAAAFAAAACMAAAAUAAAAEwAAABQAAAAeAAAAFAAAABwAAAAXAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFwAAABQAAAAUAAAAFAAAABcAAAAUAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFwAAABQAAAAUAAAAFAAAABcAAAAUAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFwAAABQAAAAUAAAAFAAAABcAAAAUAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFwAAABQAAAAUAAAAFAAAABcAAAAUAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFwAAABQAAAAUAAAAFAAAABcAAAAUAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFwAAABQAAAAUAAAAFAAAABcAAAAUAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFwAAABQAAAAUAAAAFAAAABcAAAAUAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFwAAABQAAAAUAAAAFAAAABcAAAAUAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFwAAABQAAAAUAAAAFAAAABcAAAAUAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFwAAABQAAAAUAAAAFAAAABcAAAAUAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFwAAABQAAAAUAAAAFAAAABcAAAAUAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFwAAABQAAAAUAAAAFAAAABcAAAAUAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFgAAABQAAAAUAAAAFAAAAB0AAAATAAAAEwAAAB4AAAAiAAAAFAAAABwAAAAeAAAAFgAAABMAAAAgAAAAFAAAACAAAAAUAAAAHAAAAB4AAAArAAAAKgAAABRzdGNvAAAAAAAAAAEAAAAwAAAAYnVkdGEAAABabWV0YQAAAAAAAAAhaGRscgAAAAAAAAAAbWRpcmFwcGwAAAAAAAAAAAAAAAAtaWxzdAAAACWpdG9vAAAAHWRhdGEAAAABAAAAAExhdmY1OC4yMC4xMDA=\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "test(agent,env,epochs_test,prefix='cnn_test_explore')\n",
    "HTML(display_videos('cnn_test_explore10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUMAF-Cfyh3s"
   },
   "source": [
    "***\n",
    "***\n",
    "__BONUS question__ Use the expert DQN from the previous question to generate some winning games. Train a model that mimicks its behavior. Compare the performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJFYUm-Fyh3t"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZOnNqVfxyh3t"
   },
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQN_project_MVA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
